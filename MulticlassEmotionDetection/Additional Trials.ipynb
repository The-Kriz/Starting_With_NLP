{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing Library Modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neattext.functions as nfx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing DataSet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\robolab\\NLP_setv\\MulticlassEmotionDetection\n"
     ]
    }
   ],
   "source": [
    "# Current directory\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".\\Dataset\\emotion-dataset.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                               Text\n0  neutral                                             Why ? \n1      joy    Sage Act upgrade on my to do list for tommorow.\n2  sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>Why ?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>joy</td>\n      <td>Sage Act upgrade on my to do list for tommorow.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sadness</td>\n      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "joy         11045\nsadness      6722\nfear         5410\nanger        4297\nsurprise     4062\nneutral      2254\ndisgust       856\nshame         146\nName: Emotion, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Emotion\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['BTC_ADDRESS_REGEX',\n 'CURRENCY_REGEX',\n 'CURRENCY_SYMB_REGEX',\n 'Counter',\n 'DATE_REGEX',\n 'EMAIL_REGEX',\n 'EMOJI_REGEX',\n 'HASTAG_REGEX',\n 'MASTERCard_REGEX',\n 'MD5_SHA_REGEX',\n 'MOST_COMMON_PUNCT_REGEX',\n 'NUMBERS_REGEX',\n 'PHONE_REGEX',\n 'PoBOX_REGEX',\n 'SPECIAL_CHARACTERS_REGEX',\n 'STOPWORDS',\n 'STOPWORDS_de',\n 'STOPWORDS_en',\n 'STOPWORDS_es',\n 'STOPWORDS_fr',\n 'STOPWORDS_ru',\n 'STOPWORDS_yo',\n 'STREET_ADDRESS_REGEX',\n 'TextFrame',\n 'URL_PATTERN',\n 'USER_HANDLES_REGEX',\n 'VISACard_REGEX',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__generate_text',\n '__loader__',\n '__name__',\n '__numbers_dict',\n '__package__',\n '__spec__',\n '_lex_richness_herdan',\n '_lex_richness_maas_ttr',\n 'clean_text',\n 'defaultdict',\n 'digit2words',\n 'extract_btc_address',\n 'extract_currencies',\n 'extract_currency_symbols',\n 'extract_dates',\n 'extract_emails',\n 'extract_emojis',\n 'extract_hashtags',\n 'extract_html_tags',\n 'extract_mastercard_addr',\n 'extract_md5sha',\n 'extract_numbers',\n 'extract_pattern',\n 'extract_phone_numbers',\n 'extract_postoffice_box',\n 'extract_shortwords',\n 'extract_special_characters',\n 'extract_stopwords',\n 'extract_street_address',\n 'extract_terms_in_bracket',\n 'extract_urls',\n 'extract_userhandles',\n 'extract_visacard_addr',\n 'fix_contractions',\n 'generate_sentence',\n 'hamming_distance',\n 'inverse_df',\n 'lexical_richness',\n 'markov_chain',\n 'math',\n 'nlargest',\n 'normalize',\n 'num2words',\n 'random',\n 're',\n 'read_txt',\n 'remove_accents',\n 'remove_bad_quotes',\n 'remove_btc_address',\n 'remove_currencies',\n 'remove_currency_symbols',\n 'remove_custom_pattern',\n 'remove_custom_words',\n 'remove_dates',\n 'remove_emails',\n 'remove_emojis',\n 'remove_hashtags',\n 'remove_html_tags',\n 'remove_mastercard_addr',\n 'remove_md5sha',\n 'remove_multiple_spaces',\n 'remove_non_ascii',\n 'remove_numbers',\n 'remove_phone_numbers',\n 'remove_postoffice_box',\n 'remove_puncts',\n 'remove_punctuations',\n 'remove_shortwords',\n 'remove_special_characters',\n 'remove_stopwords',\n 'remove_street_address',\n 'remove_terms_in_bracket',\n 'remove_urls',\n 'remove_userhandles',\n 'remove_visacard_addr',\n 'replace_bad_quotes',\n 'replace_currencies',\n 'replace_currency_symbols',\n 'replace_dates',\n 'replace_emails',\n 'replace_emojis',\n 'replace_numbers',\n 'replace_phone_numbers',\n 'replace_special_characters',\n 'replace_term',\n 'replace_urls',\n 'string',\n 'term_freq',\n 'to_txt',\n 'unicodedata',\n 'word_freq',\n 'word_length_freq']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "\"@Iluvmiasantos ugh babe.. hugggzzz for u .!  babe naamazed nga ako e babe e, despite nega's mas pinaramdam at fil ko ang \""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"  ugh babe.. hugggzzz for u .!  babe naamazed nga ako e babe e, despite nega's mas pinaramdam at fil ko ang \""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing user name/ Tag from the dataset\n",
    "df['Clean_Text'] = df['Text'].apply(nfx.remove_userhandles)\n",
    "df['Clean_Text'][4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(nfx.remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Removing Punctuations\n",
    "df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace =True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df['lower'] = df['Clean_Text'].str.lower()\n",
    "df['lower'][2]\n",
    "df['Wordlist'] = df['Clean_Text'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                         Clean_Text\n0  neutral                                                   \n1      joy                    Sage Act upgrade list tommorow \n2  sadness  WAY HOMEGIRL BABY FUNERAL    MAN HATE FUNERALS...\n3      joy  eye   true hazel eye and brilliant   Regular f...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Clean_Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>joy</td>\n      <td>Sage Act upgrade list tommorow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sadness</td>\n      <td>WAY HOMEGIRL BABY FUNERAL    MAN HATE FUNERALS...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>joy</td>\n      <td>eye   true hazel eye and brilliant   Regular f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessed DataSet\n",
    "df[['Emotion','Clean_Text']].head(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                              lower\n0  neutral                                                   \n1      joy                    sage act upgrade list tommorow \n2  sadness  way homegirl baby funeral    man hate funerals...\n3      joy  eye   true hazel eye and brilliant   regular f...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>lower</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>joy</td>\n      <td>sage act upgrade list tommorow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sadness</td>\n      <td>way homegirl baby funeral    man hate funerals...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>joy</td>\n      <td>eye   true hazel eye and brilliant   regular f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Emotion','lower']].head(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing machine learning packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'sage act upgrade list tommorow '"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cry\n"
     ]
    }
   ],
   "source": [
    "a = lemmatizer.lemmatize('cries')\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34792\n"
     ]
    }
   ],
   "source": [
    "print(len(df.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "for i in range(1,len(df.index)):\n",
    "    words = nltk.word_tokenize(df['lower'][i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    sentences = ' '.join(words)\n",
    "    df['Wordlist'][i] = sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lose someth go fat burger dinner dad\n"
     ]
    }
   ],
   "source": [
    "print(df['Wordlist'][28])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Assigning X and Y from Dataset\n",
    "Xfeatures = df['Wordlist']\n",
    "ylabels = df['Emotion']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Spliting Dataset\n",
    "x_train,x_test,y_train,y_test = train_test_split(Xfeatures,ylabels,test_size=0.3,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "#countvector = CountVectorizer(ngram_range=(1,1))\n",
    "countvector = CountVectorizer(ngram_range=(1,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "trainDataset = countvector.fit_transform(x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#trainDataset.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000, solver='saga')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Logistic Regression model\n",
    "logreg = LogisticRegression(solver='saga', max_iter=1000)\n",
    "logreg.fit(trainDataset, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "testDataset = countvector.transform(x_test)\n",
    "y_pred = logreg.predict(testDataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6418854186625791\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trials\n",
    "\n",
    "LogisticRegression(solver='saga', max_iter=1000) ........................... Accuracy: 0.6205211726384365\n",
    "LogisticRegression(solver='sag', max_iter=1000) ............................ Accuracy: 0.6209043878137575\n",
    "LogisticRegression(solver='newton-cg', max_iter=1000) ...................... Accuracy: 0.6209043878137575\n",
    "LogisticRegression(solver='liblinear', max_iter=1000) ...................... Accuracy: 0.6253113623299482\n",
    "\n",
    "\n",
    "gave higher Accuracy but result with custom text was bad\n",
    "countvector = CountVectorizer(ngram_range=(1,2)) + LogisticRegression(solver='liblinear', max_iter=1000)................ Accuracy: 0.6403525579612953\n",
    "Lemmatization Accuracy: 0.6432854186625791\n",
    "Stemming Accuracy: 0.6447595324774861\n",
    "without removing stop Words Accuracy: 0.6534776777160376\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hit bat', 'got mark', 'feel depress', 'hi im hari', 'ticket sold', 'loos mark', 'actual excit milan']\n",
      "['neutral' 'joy' 'sadness' 'neutral' 'neutral' 'joy' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "input= ['i will hit him with my bat','i got full mark','i am feeling depressed','hi im hari','tickets are sold out','you will loose marks','he is actually excited about milan']\n",
    "\n",
    "for i in range(0,len(input)):\n",
    "    words = nltk.word_tokenize(input[i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    sentences = ' '.join(words)\n",
    "    sentences = nfx.remove_stopwords(sentences)\n",
    "    input[i] = sentences\n",
    "print(input)\n",
    "input_countvector = countvector.transform(input)\n",
    "predictions = logreg.predict(input_countvector)\n",
    "print(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}